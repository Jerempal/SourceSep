{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9bdf04-46f1-4649-a9ab-cb40b1043c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import torchsummary as summary\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.adamw import AdamW\n",
    "from data.config import *\n",
    "from data.utils import *\n",
    "# from data.dataset import MixtureDataset, AudioMixtureDataset\n",
    "from data.dataset import AudioDataset\n",
    "from tqdm import tqdm\n",
    "from torchlibrosa.stft import STFT, ISTFT, magphase\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from metrics_loss import *\n",
    "from model.base import Base\n",
    "# load neccessary metrics/confusion matrix for multilabel classification\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "# mp.set_start_method('spawn', force=True)\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.residual_block = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_c),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_c, out_c,\n",
    "                      kernel_size=3, stride=stride, padding=1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(out_c, out_c,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "        \"\"\" Shortcut Connection \"\"\"\n",
    "        self.shortcut = nn.Conv2d(\n",
    "            in_c, out_c, kernel_size=1, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.residual_block(inputs)\n",
    "        s = self.shortcut(inputs)\n",
    "\n",
    "        skip = x + s\n",
    "        return skip\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upsampling = nn.ConvTranspose2d(\n",
    "            in_c, out_c, kernel_size=2, stride=2, padding=0, dilation=1)\n",
    "        self.residual_block = ResidualBlock(\n",
    "            out_c * 2, out_c)\n",
    "        # self.upsampling = nn.Upsample(\n",
    "        #     scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        # self.residual_block = ResidualBlock(\n",
    "        #     in_c + out_c, out_c)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        # Upsample\n",
    "        x = self.upsampling(x)\n",
    "        # Ensure x and skip have the same spatial dimensions\n",
    "        if x.shape[2:] != skip.shape[2:]:\n",
    "            x = F.interpolate(\n",
    "                x, size=(skip.shape[2], skip.shape[3]), mode='bilinear', align_corners=True)\n",
    "\n",
    "        # Concatenate\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "\n",
    "        # Residual block\n",
    "        x = self.residual_block(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResUNet(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(ResUNet, self).__init__()\n",
    "\n",
    "        \"\"\" Encoder 1 \"\"\"\n",
    "        self.encoder_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(out_c, out_c,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "        \"\"\" Shortcut Connection \"\"\"\n",
    "        self.shortcut = nn.Conv2d(in_c, out_c, kernel_size=1, padding=0)\n",
    "\n",
    "        \"\"\" Encoder 2 and 3\"\"\"\n",
    "        self.encoder_block2 = ResidualBlock(\n",
    "            out_c, out_c * 2, stride=2)\n",
    "        self.encoder_block3 = ResidualBlock(\n",
    "            out_c * 2, out_c * 4, stride=2)\n",
    "\n",
    "        \"\"\" Bridge \"\"\"\n",
    "        self.bridge = ResidualBlock(\n",
    "            out_c * 4, out_c * 8, stride=2)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.decoder_block1 = DecoderBlock(out_c * 8, out_c * 4)\n",
    "        self.decoder_block2 = DecoderBlock(out_c * 4, out_c * 2)\n",
    "        self.decoder_block3 = DecoderBlock(out_c * 2, out_c)\n",
    "\n",
    "        \"\"\" Output \"\"\"\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(out_c, 3, kernel_size=1, padding=0),\n",
    "        )\n",
    "\n",
    "        # self.classifier = nn.Sequential(\n",
    "        #     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(out_c*4, out_c*2, kernel_size=3, padding=1),\n",
    "        #     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        #     nn.ReLU(),\n",
    "\n",
    "        #     # Linear\n",
    "        #     nn.Flatten(),\n",
    "        #     nn.Linear(out_c*2 * 8 * 30, out_c*2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.3),\n",
    "        #     nn.Linear(out_c*2, out_c),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.3),\n",
    "        #     nn.Linear(out_c, 8)\n",
    "        # )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.MaxPool2d(4, 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_c*4, out_c*2, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            # nn.Linear(out_c*4 * 16 * 61, 128),\n",
    "            # nn.Linear(out_c*4 * 8 * 30, 128),\n",
    "            nn.Linear(out_c*2 * 4 * 15, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 8)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        \"\"\" Encoder 1 \"\"\"\n",
    "        encoder1 = self.encoder_block1(x)\n",
    "        s = self.shortcut(x)\n",
    "        skip1 = encoder1 + s\n",
    "\n",
    "        \"\"\" Encoder 2 and 3 \"\"\"\n",
    "        skip2 = self.encoder_block2(skip1)\n",
    "        skip3 = self.encoder_block3(skip2)\n",
    "\n",
    "        \"\"\" Bridge \"\"\"\n",
    "        bridge = self.bridge(skip3)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        decoder1 = self.decoder_block1(bridge, skip3)\n",
    "        decoder2 = self.decoder_block2(decoder1, skip2)\n",
    "        decoder3 = self.decoder_block3(decoder2, skip1)\n",
    "\n",
    "        \"\"\" Output \"\"\"\n",
    "        output = self.output(decoder3)\n",
    "\n",
    "        output_masks_dict = {\n",
    "            'mag_mask': torch.sigmoid(output[:, 0, :, :]),\n",
    "            'real_mask': torch.tanh(output[:, 1, :, :]),\n",
    "            'imag_mask': torch.tanh(output[:, 2, :, :])\n",
    "        }\n",
    "\n",
    "        class_output = self.classifier(skip3)\n",
    "        # return output, class_output\n",
    "\n",
    "        return output_masks_dict, class_output\n",
    "\n",
    "class ResUNetv2(nn.Module, Base):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(ResUNetv2, self).__init__()\n",
    "\n",
    "        window_size = 256\n",
    "        hop_size = 64\n",
    "        center = True\n",
    "        pad_mode = \"reflect\"\n",
    "        window = \"hann\"\n",
    "\n",
    "        self.output_channels = 1\n",
    "        # self.target_sources_num = 1\n",
    "        self.K = 3\n",
    "\n",
    "        # downsample ratio\n",
    "        self.time_downsample_ratio = 2**3  # number of encoder layers\n",
    "\n",
    "        self.stft = STFT(\n",
    "            n_fft=window_size,\n",
    "            hop_length=hop_size,\n",
    "            win_length=window_size,\n",
    "            window=window,\n",
    "            center=center,\n",
    "            pad_mode=pad_mode,\n",
    "            freeze_parameters=True,\n",
    "        )\n",
    "\n",
    "        self.istft = ISTFT(\n",
    "            n_fft=window_size,\n",
    "            hop_length=hop_size,\n",
    "            win_length=window_size,\n",
    "            window=window,\n",
    "            center=center,\n",
    "            pad_mode=pad_mode,\n",
    "            freeze_parameters=True,\n",
    "        )\n",
    "\n",
    "        \"\"\" Encoder 1 \"\"\"\n",
    "        self.encoder_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(out_c, out_c,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "        \"\"\" Shortcut Connection \"\"\"\n",
    "        self.shortcut = nn.Conv2d(in_c, out_c, kernel_size=1, padding=0)\n",
    "        # self.shortcut = nn.Conv2d(in_c, out_c, kernel_size=1, padding=\"same\"),\n",
    "\n",
    "        \"\"\" Encoder 2 and 3\"\"\"\n",
    "        self.encoder_block2 = ResidualBlock(out_c, out_c * 2, stride=2)\n",
    "        self.encoder_block3 = ResidualBlock(out_c * 2, out_c * 4, stride=2)\n",
    "\n",
    "        \"\"\" Bridge \"\"\"\n",
    "        self.bridge = ResidualBlock(\n",
    "            out_c * 4, out_c * 8, stride=2)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.decoder_block1 = DecoderBlock(out_c * 8, out_c * 4)\n",
    "        self.decoder_block2 = DecoderBlock(out_c * 4, out_c * 2)\n",
    "        self.decoder_block3 = DecoderBlock(out_c * 2, out_c)\n",
    "\n",
    "        \"\"\" Output \"\"\"\n",
    "        # self.last_layer = nn.Sequential(\n",
    "        #     # nn.Conv2d(out_c, 1, kernel_size=1, padding='same'),\n",
    "        #     nn.Conv2d(out_c, 1, kernel_size=1, padding=0),\n",
    "        # )\n",
    "\n",
    "        self.after_conv = nn.Conv2d(\n",
    "            in_channels=out_c,\n",
    "            out_channels=self.output_channels * self.K,\n",
    "            kernel_size=(1, 1),\n",
    "            stride=(1, 1),\n",
    "            padding=(0, 0),\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "    def feature_maps_to_wav(\n",
    "        self,\n",
    "        input_tensor: torch.Tensor,\n",
    "        sp: torch.Tensor,\n",
    "        sin_in: torch.Tensor,\n",
    "        cos_in: torch.Tensor,\n",
    "        audio_length: int,\n",
    "    ) -> torch.Tensor:\n",
    "        r\"\"\"Convert feature maps to waveform.\n",
    "\n",
    "        Args:\n",
    "            input_tensor: (batch_size, target_sources_num * output_channels * self.K, time_steps, freq_bins)\n",
    "            sp: (batch_size, input_channels, time_steps, freq_bins)\n",
    "            sin_in: (batch_size, input_channels, time_steps, freq_bins)\n",
    "            cos_in: (batch_size, input_channels, time_steps, freq_bins)\n",
    "\n",
    "            (There is input_channels == output_channels for the source separation task.)\n",
    "\n",
    "        Outputs:\n",
    "            waveform: (batch_size, target_sources_num * output_channels, segment_samples)\n",
    "        \"\"\"\n",
    "        batch_size, _, time_steps, freq_bins = input_tensor.shape\n",
    "\n",
    "        x = input_tensor.reshape(\n",
    "            batch_size,\n",
    "            # self.target_sources_num,\n",
    "            self.output_channels,\n",
    "            self.K,\n",
    "            time_steps,\n",
    "            freq_bins,\n",
    "        )\n",
    "        # x: (batch_size, target_sources_num, output_channels, self.K, time_steps, freq_bins)\n",
    "\n",
    "        # mask_mag = torch.sigmoid(x[:, :, :, 0, :, :])\n",
    "        # _mask_real = torch.tanh(x[:, :, :, 1, :, :])\n",
    "        # _mask_imag = torch.tanh(x[:, :, :, 2, :, :])\n",
    "        \n",
    "        mask_mag = torch.sigmoid(x[:, :, 0, :, :])\n",
    "        _mask_real = torch.tanh(x[:, :, 1, :, :])\n",
    "        _mask_imag = torch.tanh(x[:, :, 2, :, :])\n",
    "        # print(mask_mag.shape)\n",
    "        # linear_mag = torch.tanh(x[:, :, :, 3, :, :])\n",
    "        _, mask_cos, mask_sin = magphase(_mask_real, _mask_imag)\n",
    "        # print(mask_mag.shape)\n",
    "        # mask_cos, mask_sin: (batch_size, target_sources_num, output_channels, time_steps, freq_bins)\n",
    "\n",
    "        # Y = |Y|cos∠Y + j|Y|sin∠Y\n",
    "        #   = |Y|cos(∠X + ∠M) + j|Y|sin(∠X + ∠M)\n",
    "        #   = |Y|(cos∠X cos∠M - sin∠X sin∠M) + j|Y|(sin∠X cos∠M + cos∠X sin∠M)\n",
    "        out_cos = (\n",
    "            cos_in[:, :, :, :] * mask_cos -\n",
    "            sin_in[:, :, :, :] * mask_sin\n",
    "        )\n",
    "        out_sin = (\n",
    "            sin_in[:, :, :, :] * mask_cos +\n",
    "            cos_in[:, :, :, :] * mask_sin\n",
    "        )\n",
    "        # print(out_cos.shape)\n",
    "        # out_cos: (batch_size, target_sources_num, output_channels, time_steps, freq_bins)\n",
    "        # out_sin: (batch_size, target_sources_num, output_channels, time_steps, freq_bins)\n",
    "\n",
    "        # Calculate |Y|.\n",
    "        out_mag = F.relu_(sp[:, :, :, :] * mask_mag)\n",
    "        #print(out_mag.shape)\n",
    "        # out_mag: (batch_size, target_sources_num, output_channels, time_steps, freq_bins)\n",
    "\n",
    "        # Calculate Y_{real} and Y_{imag} for ISTFT.\n",
    "        out_real = out_mag * out_cos\n",
    "        out_imag = out_mag * out_sin\n",
    "        # out_real, out_imag: (batch_size, target_sources_num, output_channels, time_steps, freq_bins)\n",
    "        #print(out_real.shape)\n",
    "        # Reformat shape to (N, 1, time_steps, freq_bins) for ISTFT where\n",
    "        # N = batch_size * target_sources_num * output_channels\n",
    "        # shape = (batch_size * self.target_sources_num *\n",
    "                #  self.output_channels, 1, time_steps, freq_bins)\n",
    "\n",
    "        shape = (batch_size * self.output_channels, 1, time_steps, freq_bins)\n",
    "        shape = (batch_size, 1, time_steps, freq_bins)\n",
    "        #print(shape)\n",
    "        \n",
    "        out_real = out_real.reshape(shape)\n",
    "        out_imag = out_imag.reshape(shape)\n",
    "        #print(out_real.shape)\n",
    "        # ISTFT.\n",
    "        x = self.istft(out_real, out_imag, audio_length)\n",
    "        # (batch_size * target_sources_num * output_channels, segments_num)\n",
    "\n",
    "        # Reshape.\n",
    "        # waveform = x.reshape(batch_size, self.target_sources_num * self.output_channels, audio_length)\n",
    "        waveform = x.reshape(batch_size, self.output_channels, audio_length)\n",
    "        # (batch_size, target_sources_num * output_channels, segments_num)\n",
    "        return waveform\n",
    "\n",
    "    def forward(self, mixtures):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: (batch_size, segment_samples)\n",
    "\n",
    "        Outputs:\n",
    "            output_dict: {\n",
    "            'wav': (batch_size, segment_samples),\n",
    "            'sp': (batch_size, channels_num, time_steps, freq_bins)}\n",
    "        \"\"\"\n",
    "        mag, cos_in, sin_in = self.wav_to_spectrogram_phase(mixtures)\n",
    "        x = mag\n",
    "        # Pad spectrogram to be evenly divided by downsample ratio.\n",
    "        origin_len = x.shape[2]\n",
    "        pad_len = (\n",
    "            int(np.ceil(x.shape[2] / self.time_downsample_ratio)\n",
    "                ) * self.time_downsample_ratio\n",
    "            - origin_len\n",
    "        )\n",
    "        x = F.pad(x, pad=(0, 0, 0, pad_len))\n",
    "\n",
    "        \"\"\"(batch_size, channels, padded_time_steps, freq_bins)\"\"\"\n",
    "        # Let frequency bins be evenly divided by 2, e.g., 489 -> 488.\n",
    "        x = x[..., 0: x.shape[-1] - 1]  # (bs, channels, T, F)\n",
    "        # UNet\n",
    "        \"\"\" Encoder 1 \"\"\"\n",
    "        encoder1 = self.encoder_block1(x)\n",
    "        s = self.shortcut(x)\n",
    "        skip1 = encoder1 + s\n",
    "\n",
    "        \"\"\" Encoder 2 and 3 \"\"\"\n",
    "        skip2 = self.encoder_block2(skip1)\n",
    "        skip3 = self.encoder_block3(skip2)\n",
    "\n",
    "        \"\"\" Bridge \"\"\"\n",
    "        bridge = self.bridge(skip3)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        decoder1 = self.decoder_block1(bridge, skip3)\n",
    "        decoder2 = self.decoder_block2(decoder1, skip2)\n",
    "        decoder3 = self.decoder_block3(decoder2, skip1)\n",
    "\n",
    "        \"\"\" Output \"\"\"\n",
    "        # output = self.last_layer(decoder3)\n",
    "\n",
    "        x = self.after_conv(decoder3)\n",
    "\n",
    "        # (batch_size, target_sources_num * output_channels * self.K, T, F')\n",
    "\n",
    "        # Recover shape\n",
    "        x = F.pad(x, pad=(0, 1))\n",
    "        x = x[:, :, 0:origin_len, :]\n",
    "\n",
    "        audio_length = mixtures.shape[2]\n",
    "        # Recover each subband spectrograms to subband waveforms. Then synthesis\n",
    "        # the subband waveforms to a waveform.\n",
    "        separated_audio = self.feature_maps_to_wav(\n",
    "            input_tensor=x,\n",
    "            # input_tensor: (batch_size, target_sources_num * output_channels * self.K, T, F')\n",
    "            sp=mag,\n",
    "            # sp: (batch_size, input_channels, T, F')\n",
    "            sin_in=sin_in,\n",
    "            # sin_in: (batch_size, input_channels, T, F')\n",
    "            cos_in=cos_in,\n",
    "            # cos_in: (batch_size, input_channels, T, F')\n",
    "            audio_length=audio_length,\n",
    "        )\n",
    "        # （batch_size, target_sources_num * output_channels, subbands_num, segment_samples)\n",
    "\n",
    "        output_dict = {'waveform': separated_audio}\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e85741-363d-4503-bd43-641b3d5514d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Define the multi-task loss function\n",
    "\n",
    "\n",
    "def multi_task_loss(separation_output, classification_output, true_percussion, true_class, alpha=0.7, beta=0.3, spectrogram_loss=False):\n",
    "\n",
    "    if spectrogram_loss == False:\n",
    "        mse_loss = nn.MSELoss()\n",
    "        separation_loss = mse_loss(separation_output, true_percussion)\n",
    "\n",
    "    else:\n",
    "        separation_loss = spectral_loss(separation_output, true_percussion)\n",
    "\n",
    "    # classification_loss = nn.CrossEntropyLoss()(classification_output, true_class)\n",
    "    classification_loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1, 2, 1, 1, 1, 1, 1, 1]).to(\"cuda\"))(\n",
    "        classification_output, true_class)\n",
    "\n",
    "    loss = alpha * separation_loss + beta * classification_loss\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df40237b-0746-4555-80b6-86c9488ae1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Load metadata\n",
    "metadata = pd.read_csv(os.path.join(\n",
    "    DATASET_MIX_AUDIO_PATH, \"metadata.csv\"))\n",
    "\n",
    "# define the train, validation and test sets\n",
    "\n",
    "# dataset = MixtureDataset(metadata_file=metadata, k=0.6,\n",
    "#                          noise_class=None)\n",
    "# dataset = AudioMixtureDataset(metadata_file=metadata, k=0.4,\n",
    "#                               noise_class='siren')\n",
    "# dataset = AudioMixtureDataset(metadata_file=metadata, k=None, noise_class=None)\n",
    "\n",
    "# dataset = AudioDataset(metadata_file=metadata, noise_classes=[\n",
    "#                                           'engine_idling', 'air_conditioner'], random_noise=True)\n",
    "dataset = AudioDataset(metadata_file=metadata, random_noise=True, classify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfd2bf8-fcbd-463d-b805-4788bcedcbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# when using the saved indices\n",
    "train_indices = np.load('train_indices.npy')\n",
    "val_indices = np.load('val_indices.npy')\n",
    "test_indices = np.load('test_indices.npy')\n",
    "\n",
    "# train_indices = np.load('train_indices_engine_air.npy')\n",
    "# val_indices = np.load('val_indices_engine_air.npy')\n",
    "# test_indices = np.load('test_indices_engine_air.npy')\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "# train_loader = DataLoader(dataset, sampler=train_sampler, batch_size=32, num_workers=2, persistent_workers=True, prefetch_factor=2)\n",
    "# val_loader = DataLoader(dataset, sampler=val_sampler,\n",
    "#                         batch_size=32, num_workers=2, persistent_workers=True, prefetch_factor=2)\n",
    "# test_loader = DataLoader(dataset, sampler=test_sampler,\n",
    "#                          batch_size=32, num_workers=2, persistent_workers=True, prefetch_factor=2)\n",
    "\n",
    "train_loader = DataLoader(dataset, sampler=train_sampler, batch_size=20)\n",
    "val_loader = DataLoader(dataset, sampler=val_sampler, batch_size=20)\n",
    "test_loader = DataLoader(dataset, sampler=test_sampler, batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7595584b-9ab7-4101-a0b9-a3ba22d86a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Define the model, optimizer and loss function\n",
    "# model = MultiTaskResUNet(num_noise_classes=8).to(\"cuda\")\n",
    "# model = ResUNet(in_c=1, out_c=16).to(\"cuda\")\n",
    "model = ResUNetv2(in_c=1, out_c=16).to(\"cuda\")\n",
    "optimizer = AdamW(model.parameters(), lr=0.001, amsgrad=True, fused=True)\n",
    "# optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "# criterion = multi_task_loss\n",
    "criterion = nn.MSELoss()\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f3538a-d645-42d5-a150-bf3bdb90af69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Training Loss: 0.0034:   0%|\u001b[32m          \u001b[0m| 1/581 [00:01<16:04,  1.66s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jejep\\Desktop\\STAGE\\Projects\\SourceSep\\maincode.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     20\u001b[0m train_bar \u001b[39m=\u001b[39m tqdm(train_loader, desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m                 \u001b[39mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m Training Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, colour\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgreen\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_bar):\n\u001b[0;32m     23\u001b[0m     \u001b[39m# Zero the parameter gradients\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     26\u001b[0m     \u001b[39m# Move data to device\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jejep\\anaconda3\\envs\\ProjectEnv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jejep\\anaconda3\\envs\\ProjectEnv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:700\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    698\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    699\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 700\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[0;32m    701\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    702\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    703\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable\n\u001b[0;32m    704\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    705\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    706\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\jejep\\anaconda3\\envs\\ProjectEnv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:756\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    754\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    755\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 756\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    757\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    758\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\jejep\\anaconda3\\envs\\ProjectEnv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\jejep\\Desktop\\STAGE\\Projects\\SourceSep\\data\\dataset.py:155\u001b[0m, in \u001b[0;36mAudioDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    152\u001b[0m noise_name \u001b[39m=\u001b[39m [row[\u001b[39m'\u001b[39m\u001b[39mnoise_file\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata[\u001b[39m'\u001b[39m\u001b[39mnoise_class\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39misin(noise_l)]\u001b[39m.\u001b[39mto_dict(orient\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrecords\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m    154\u001b[0m \u001b[39m#create channel dim\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m mixture_audio \u001b[39m=\u001b[39m mixture_audio\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m    156\u001b[0m percussion_audio \u001b[39m=\u001b[39m percussion_audio\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m    158\u001b[0m \u001b[39m# noise name not always same size\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Train the model\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = np.inf\n",
    "patience = 5\n",
    "num_epochs = 5\n",
    "\n",
    "# model, optimizer, start_epoch, loss = load_checkpoint(model, optimizer, checkpoint_dir='checkpoint', filename='checkpoint_air_engine_epoch_3.pth')\n",
    "# model, optimizer, start_epoch, loss = load_checkpoint(model, optimizer, checkpoint_dir='checkpoint', filename='checkpoint_air_engine_spectralv1_epoch_2.pth')\n",
    "# model, optimizer, start_epoch, loss = load_checkpoint(\n",
    "#     model, optimizer, checkpoint_dir='checkpoint', filename='checkpoint_spectral_epoch_2.pth')\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {\n",
    "                     epoch + 1}/{num_epochs} Training Loss: {train_loss:.4f}\", colour='green')\n",
    "    for i, batch in enumerate(train_bar):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move data to device\n",
    "        mixture = batch['mixture_audio'].to(device)\n",
    "        true_percussion = batch['percussion_audio'].to(device)\n",
    "        # mix_stft = batch['mix_stft'].to(device)\n",
    "        # true_percussion_stft = batch['perc_stft'].to(device)\n",
    "\n",
    "        # true_class = batch['noise_class'].to(device)\n",
    "        # ici true class est un tensor de taille (batch_size, 8) avec des 0 et des 1 pour les classes présentes et absentes\n",
    "        # true_class = batch['noise_labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        # output, class_output = model(torch.abs(mix_stft))\n",
    "        output_waveform = model(mixture)\n",
    "        output_waveform = output_waveform['waveform']\n",
    "        \n",
    "        # Reconstruct the complex spectrogram\n",
    "        # sep_output = SpectrogramReconstructor().reconstruct(\n",
    "        #     output['mag_mask'], output['real_mask'], output['imag_mask'], mix_stft)\n",
    "        # percussion_sep = istft(sep_output, n_fft=256, hop_length=64)\n",
    "\n",
    "        # Calculate the loss\n",
    "        # loss = criterion(percussion_sep, class_output, true_percussion, true_class)\n",
    "        # loss = criterion(sep_output, class_output, true_percussion_stft,\n",
    "        #                  true_class, alpha=0.7, beta=0.3, spectrogram_loss=True)\n",
    "\n",
    "        loss = criterion(output_waveform, true_percussion)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # else we calculate log spectral loss so we need to calculate the stft of the separated percussion (sep_output is the complex spectrogram of the separated percussion)\n",
    "        # true_percussion_stft = torch.stft(true_percussion, n_fft=256, hop_length=64, win_length=256, window=torch.hann_window(window_length=256, device=device), return_complex=True)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_bar.set_description(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs} Training Loss: {train_loss/(i+1):.4f}\")\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    # confusion matrix for multilabel classification\n",
    "    # correct = 0\n",
    "    # total = 0\n",
    "    # all_preds = []\n",
    "    # all_labels = []\n",
    "\n",
    "    val_bar = tqdm(val_loader, desc=f\"Epoch {\n",
    "        epoch + 1}/{num_epochs} Validation Loss: {val_loss:.4f}\", colour='red')\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_bar):\n",
    "            # Move data to device\n",
    "            mixture = batch['mixture_audio'].to(device)\n",
    "            # mix_stft = batch['mix_stft'].to(device)\n",
    "            true_percussion = batch['percussion_audio'].to(device)\n",
    "            # true_percussion_stft = batch['perc_stft'].to(device)\n",
    "            # true_class = batch['noise_labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            # output, class_output = model(torch.abs(mix_stft))\n",
    "            output_waveform = model(mixture)\n",
    "            output_waveform = output_waveform['waveform']\n",
    "            # Reconstruct the complex spectrogram\n",
    "            # sep_output = SpectrogramReconstructor().reconstruct(\n",
    "            #     output['mag_mask'], output['real_mask'], output['imag_mask'], mix_stft)\n",
    "            # percussion_sep = istft(sep_output, n_fft=256, hop_length=64)\n",
    "\n",
    "            # Calculate the loss\n",
    "            # loss = criterion(percussion_sep, class_output, true_percussion, true_class)\n",
    "            # loss = criterion(sep_output, class_output, true_percussion_stft,\n",
    "            #                  true_class, alpha=0.7, beta=0.3, spectrogram_loss=True)\n",
    "\n",
    "            loss = criterion(output_waveform, true_percussion)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # # Calculate multi-label classification accuracy\n",
    "            # predicted = (torch.sigmoid(class_output) > 0.5).float()\n",
    "            # # Total for a multi-label classification:\n",
    "            # total += true_class.size(0) * true_class.size(1)\n",
    "            # correct += (predicted == true_class).float().sum().item()\n",
    "\n",
    "            # all_preds.extend(predicted.cpu().numpy())\n",
    "            # all_labels.extend(true_class.cpu().numpy())\n",
    "\n",
    "            val_bar.set_description(\n",
    "                f\"Epoch {epoch + 1}/{num_epochs} Validation Loss: {val_loss/(i+1):.4f}\")\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        # accuracy = correct / total\n",
    "\n",
    "        # print(f\"Epoch {epoch + 1}/{num_epochs} Validation Loss: {\n",
    "        #       val_loss:.4f}, Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # # confusion matrix\n",
    "        # labels = ['air_conditioner', 'car_horn', 'children_playing',\n",
    "        #           'dog_bark', 'drilling', 'engine_idling', 'siren', 'jackhammer']\n",
    "        # cm = multilabel_confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "        # # plot confusion matrix\n",
    "        # from sklearn.metrics import ConfusionMatrixDisplay\n",
    "        # fig, ax = plt.subplots(2, 4, figsize=(20, 10))\n",
    "        # for i in range(8):\n",
    "        #     disp = ConfusionMatrixDisplay(cm[i], display_labels=[0, 1])\n",
    "        #     disp.plot(ax=ax[i//4, i % 4])\n",
    "        #     disp.ax_.set_title(labels[i])\n",
    "        # plt.show()\n",
    "\n",
    "        # # classification report multilabel\n",
    "        # print(classification_report(all_labels, all_preds,\n",
    "        #       target_names=labels, zero_division=0))\n",
    "\n",
    "        # save checkpoint\n",
    "        save_checkpoint(model, optimizer, epoch, train_loss, val_loss, checkpoint_dir='checkpoint',\n",
    "                        filename='checkpoint_v4_epoch_{}'.format(epoch))\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), 'best_model.pth')\n",
    "            torch.save(model.state_dict(), 'best_model_v4.pth')\n",
    "            print(\"Model improved. Saving the model...\")\n",
    "\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience == 0:\n",
    "                print(\"Early stopping\")\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

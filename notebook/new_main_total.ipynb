{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d02d071-3f69-459e-9e7b-2bc0fdd5fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import librosa.display\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from data.config import *\n",
    "from data.utils import *\n",
    "# from data.dataset import MixtureDataset, AudioMixtureDataset\n",
    "from data.dataset import AudioDataset\n",
    "from tqdm import tqdm\n",
    "from torchlibrosa.stft import STFT, ISTFT, magphase\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from metrics_loss import *\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "# mp.set_start_method('spawn', force=True)\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f355af2-ba41-4248-946a-4f1421259fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.residual_block = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_c),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_c, out_c,\n",
    "                      kernel_size=3, stride=stride, padding=1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(out_c, out_c,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "        \"\"\" Shortcut Connection \"\"\"\n",
    "        self.shortcut = nn.Conv2d(\n",
    "            in_c, out_c, kernel_size=1, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.residual_block(inputs)\n",
    "        s = self.shortcut(inputs)\n",
    "\n",
    "        skip = x + s\n",
    "        return skip\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upsampling = nn.ConvTranspose2d(\n",
    "            in_c, out_c, kernel_size=2, stride=2, padding=0, dilation=1)\n",
    "        self.residual_block = ResidualBlock(\n",
    "            out_c * 2, out_c)\n",
    "        # self.upsampling = nn.Upsample(\n",
    "        #     scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        # self.residual_block = ResidualBlock(\n",
    "        #     in_c + out_c, out_c)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        # Upsample\n",
    "        x = self.upsampling(x)\n",
    "        # Ensure x and skip have the same spatial dimensions\n",
    "        if x.shape[2:] != skip.shape[2:]:\n",
    "            x = F.interpolate(\n",
    "                x, size=(skip.shape[2], skip.shape[3]), mode='bilinear', align_corners=True)\n",
    "\n",
    "        # Concatenate\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "\n",
    "        # Residual block\n",
    "        x = self.residual_block(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResUNet(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(ResUNet, self).__init__()\n",
    "\n",
    "        \"\"\" Encoder 1 \"\"\"\n",
    "        self.encoder_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(out_c, out_c,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "        \"\"\" Shortcut Connection \"\"\"\n",
    "        self.shortcut = nn.Conv2d(in_c, out_c, kernel_size=1, padding=0)\n",
    "\n",
    "        \"\"\" Encoder 2 and 3\"\"\"\n",
    "        self.encoder_block2 = ResidualBlock(\n",
    "            out_c, out_c * 2, stride=2)\n",
    "        self.encoder_block3 = ResidualBlock(\n",
    "            out_c * 2, out_c * 4, stride=2)\n",
    "\n",
    "        \"\"\" Bridge \"\"\"\n",
    "        self.bridge = ResidualBlock(\n",
    "            out_c * 4, out_c * 8, stride=2)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.decoder_block1 = DecoderBlock(out_c * 8, out_c * 4)\n",
    "        self.decoder_block2 = DecoderBlock(out_c * 4, out_c * 2)\n",
    "        self.decoder_block3 = DecoderBlock(out_c * 2, out_c)\n",
    "\n",
    "        \"\"\" Output \"\"\"\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(out_c, 3, kernel_size=1, padding=0),\n",
    "        )\n",
    "\n",
    "        # # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 8),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        \"\"\" Encoder 1 \"\"\"\n",
    "        encoder1 = self.encoder_block1(x)\n",
    "        s = self.shortcut(x)\n",
    "        skip1 = encoder1 + s\n",
    "\n",
    "        \"\"\" Encoder 2 and 3 \"\"\"\n",
    "        skip2 = self.encoder_block2(skip1)\n",
    "        skip3 = self.encoder_block3(skip2)\n",
    "\n",
    "        \"\"\" Bridge \"\"\"\n",
    "        bridge = self.bridge(skip3)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        decoder1 = self.decoder_block1(bridge, skip3)\n",
    "        decoder2 = self.decoder_block2(decoder1, skip2)\n",
    "        decoder3 = self.decoder_block3(decoder2, skip1)\n",
    "\n",
    "        \"\"\" Output \"\"\"\n",
    "        output = self.output(decoder3)\n",
    "\n",
    "        output_masks_dict = {\n",
    "            'mag_mask': torch.sigmoid(output[:, 0, :, :]),\n",
    "            'real_mask': torch.tanh(output[:, 1, :, :]),\n",
    "            'imag_mask': torch.tanh(output[:, 2, :, :])\n",
    "        }\n",
    "\n",
    "        class_output = self.classifier(skip3)\n",
    "\n",
    "        return output_masks_dict, class_output\n",
    "\n",
    "\n",
    "# class MultiTaskResUNet(nn.Module):\n",
    "#     def __init__(self, num_noise_classes):\n",
    "#         super().__init__()\n",
    "#         self.resunet = ResUNet(in_c=1, out_c=32)\n",
    "\n",
    "#         # Classification head\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.AdaptiveAvgPool2d((1, 1)),\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(64, num_noise_classes),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "\n",
    "#         output, skip3 = self.resunet(x)\n",
    "#         x = self.classifier(skip3)\n",
    "\n",
    "#         return output, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a569d08-bf86-433d-9a38-64fd74fee07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Define the multi-task loss function\n",
    "\n",
    "\n",
    "def multi_task_loss(separation_output, classification_output, true_percussion, true_class, alpha=0.7, beta=0.3, spectrogram_loss=False):\n",
    "\n",
    "    if spectrogram_loss == False:\n",
    "        mse_loss = nn.MSELoss()\n",
    "        separation_loss = mse_loss(separation_output, true_percussion)\n",
    "\n",
    "    else:\n",
    "        separation_loss = spectral_loss(separation_output, true_percussion)\n",
    "\n",
    "    # classification_loss = nn.CrossEntropyLoss()(classification_output, true_class)\n",
    "    classification_loss = nn.BCEWithLogitsLoss()(classification_output, true_class)\n",
    "\n",
    "    loss = alpha * separation_loss + beta * classification_loss\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d621ee46-36ad-4465-ba5f-edfce11e9732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Load metadata\n",
    "metadata = pd.read_csv(os.path.join(\n",
    "    DATASET_MIX_AUDIO_PATH, \"metadata.csv\"))\n",
    "\n",
    "# define the train, validation and test sets\n",
    "\n",
    "# dataset = MixtureDataset(metadata_file=metadata, k=0.6,\n",
    "#                          noise_class=None)\n",
    "# dataset = AudioMixtureDataset(metadata_file=metadata, k=0.4,\n",
    "#                               noise_class='siren')\n",
    "# dataset = AudioMixtureDataset(metadata_file=metadata, k=None, noise_class=None)\n",
    "\n",
    "# dataset = AudioDataset(metadata_file=metadata, noise_classes=[\n",
    "#                                           'engine_idling', 'air_conditioner'], random_noise=True)\n",
    "dataset = AudioDataset(metadata_file=metadata, random_noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50aae5d-ee51-40b1-ab4a-0c63d2b4c1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# when using the saved indices\n",
    "train_indices = np.load('train_indices.npy')\n",
    "val_indices = np.load('val_indices.npy')\n",
    "test_indices = np.load('test_indices.npy')\n",
    "\n",
    "# train_indices = np.load('train_indices_engine_air.npy')\n",
    "# val_indices = np.load('val_indices_engine_air.npy')\n",
    "# test_indices = np.load('test_indices_engine_air.npy')\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "# train_loader = DataLoader(dataset, sampler=train_sampler, batch_size=32, num_workers=2, persistent_workers=True, prefetch_factor=2)\n",
    "# val_loader = DataLoader(dataset, sampler=val_sampler,\n",
    "#                         batch_size=32, num_workers=2, persistent_workers=True, prefetch_factor=2)\n",
    "# test_loader = DataLoader(dataset, sampler=test_sampler,\n",
    "#                          batch_size=32, num_workers=2, persistent_workers=True, prefetch_factor=2)\n",
    "\n",
    "train_loader = DataLoader(dataset, sampler=train_sampler, batch_size=32)\n",
    "val_loader = DataLoader(dataset, sampler=val_sampler, batch_size=32)\n",
    "test_loader = DataLoader(dataset, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3f7af7-55f7-44e6-b1d3-e00deba6edc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "class SpectrogramReconstructor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def magphase(self, real, imag):\n",
    "        mag = (real ** 2 + imag ** 2) ** 0.5\n",
    "        cos = real / torch.clamp(mag, 1e-10, np.inf)\n",
    "        sin = imag / torch.clamp(mag, 1e-10, np.inf)\n",
    "\n",
    "        return mag, cos, sin\n",
    "\n",
    "    def reconstruct(self, mag_mask, real_mask, imag_mask, mix_stft):\n",
    "\n",
    "        mix_mag, mix_cos, mix_sin = self.magphase(mix_stft.real, mix_stft.imag)\n",
    "        _, mask_cos, mask_sin = self.magphase(real_mask, imag_mask)\n",
    "\n",
    "        # calculate the |Y| = |M| * |X|\n",
    "        estimated_mag = mag_mask * mix_mag\n",
    "\n",
    "        # Reconstruct the complex spectrogram\n",
    "        Y_real = estimated_mag * (mask_cos * mix_cos - mask_sin * mix_sin)\n",
    "        Y_imag = estimated_mag * (mask_cos * mix_sin + mask_sin * mix_cos)\n",
    "        sep_output = torch.complex(Y_real, Y_imag)\n",
    "\n",
    "        return sep_output\n",
    "\n",
    "\n",
    "# ISTFT conversion function\n",
    "\n",
    "\n",
    "def istft(sep_output, n_fft, hop_length):\n",
    "\n",
    "    y = torch.istft(\n",
    "        sep_output, n_fft, hop_length, window=torch.hann_window(256, device='cuda'), length=31248)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93cc3ef-9f6b-4008-a26a-bfb29a5dc7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Define the model, optimizer and loss function\n",
    "# model = MultiTaskResUNet(num_noise_classes=8).to(\"cuda\")\n",
    "model = ResUNet(in_c=1, out_c=32).to(\"cuda\")\n",
    "optimizer = AdamW(model.parameters(), lr=0.001, amsgrad=True)\n",
    "# optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "criterion = multi_task_loss\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ef2152-22f0-40a3-86b9-6f896fad3488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training Loss: 1.1680: 100%|\u001b[32m██████████\u001b[0m| 363/363 [07:49<00:00,  1.29s/it]\n",
      "Epoch 1/2 Validation Loss: 1.1532: 100%|\u001b[31m██████████\u001b[0m| 121/121 [01:19<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Training Loss: 1.1680, Training Accuracy: 0.8120, Validation Loss: 1.1532, Validation Accuracy: 0.8140\n",
      "Checkpoint saved at 'checkpoint\\checkpoint_spectral_epoch_1.pth'\n",
      "Model improved. Saving the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training Loss: 1.0991: 100%|\u001b[32m██████████\u001b[0m| 363/363 [10:30<00:00,  1.74s/it]\n",
      "Epoch 2/2 Validation Loss: 1.0821: 100%|\u001b[31m██████████\u001b[0m| 121/121 [01:15<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 Training Loss: 1.0991, Training Accuracy: 0.8147, Validation Loss: 1.0821, Validation Accuracy: 0.8163\n",
      "Checkpoint saved at 'checkpoint\\checkpoint_spectral_epoch_2.pth'\n",
      "Model improved. Saving the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Train the model\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = np.inf\n",
    "patience = 5\n",
    "num_epochs = 2\n",
    "\n",
    "# model, optimizer, start_epoch, loss = load_checkpoint(model, optimizer, checkpoint_dir='checkpoint', filename='checkpoint_air_engine_epoch_3.pth')\n",
    "# model, optimizer, start_epoch, loss = load_checkpoint(model, optimizer, checkpoint_dir='checkpoint', filename='checkpoint_air_engine_spectralv1_epoch_2.pth')\n",
    "start_epoch = 0\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {\n",
    "                     epoch + 1}/{num_epochs} Training Loss: {train_loss:.4f}\", colour='green')\n",
    "    for i, batch in enumerate(train_bar):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move data to device\n",
    "        # mixture = batch['mixture_audio'].to(device)\n",
    "        # true_percussion = batch['percussion_audio'].to(device)\n",
    "        mix_stft = batch['mix_stft'].to(device)\n",
    "        true_percussion_stft = batch['perc_stft'].to(device)\n",
    "\n",
    "        # true_class = batch['noise_class'].to(device)\n",
    "        # ici true class est un tensor de taille (batch_size, 8) avec des 0 et des 1 pour les classes présentes et absentes\n",
    "        true_class = batch['noise_labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output, class_output = model(torch.abs(mix_stft))\n",
    "\n",
    "        # Reconstruct the complex spectrogram\n",
    "        sep_output = SpectrogramReconstructor().reconstruct(\n",
    "            output['mag_mask'], output['real_mask'], output['imag_mask'], mix_stft)\n",
    "        # percussion_sep = istft(sep_output, n_fft=256, hop_length=64)\n",
    "\n",
    "        # Calculate the loss\n",
    "        # loss = criterion(percussion_sep, class_output, true_percussion, true_class)\n",
    "        loss=criterion(sep_output, class_output, true_percussion_stft,\n",
    "                       true_class, alpha=0.7, beta=0.3, spectrogram_loss=True)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # else we calculate log spectral loss so we need to calculate the stft of the separated percussion (sep_output is the complex spectrogram of the separated percussion)\n",
    "        # true_percussion_stft = torch.stft(true_percussion, n_fft=256, hop_length=64, win_length=256, window=torch.hann_window(window_length=256, device=device), return_complex=True)\n",
    "\n",
    "        predicted = (F.sigmoid(class_output) >= 0.5).float()\n",
    "        correct += (predicted == true_class).float().sum().item()\n",
    "        total += true_class.numel()\n",
    "        \n",
    "        train_loss += loss.item()    \n",
    "        train_bar.set_description(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs} Training Loss: {train_loss/(i+1):.4f}\")\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    accuracy=correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss=0\n",
    "    total=0\n",
    "    correct=0\n",
    "\n",
    "    val_bar=tqdm(val_loader, desc=f\"Epoch {\n",
    "                   epoch + 1}/{num_epochs} Validation Loss: {val_loss:.4f}\", colour='red')\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_bar):\n",
    "            # Move data to device\n",
    "            # mixture = batch['mixture_audio'].to(device)\n",
    "            mix_stft=batch['mix_stft'].to(device)\n",
    "            # true_percussion = batch['percussion_audio'].to(device)\n",
    "            true_percussion_stft=batch['perc_stft'].to(device)\n",
    "            true_class=batch['noise_labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output, class_output=model(torch.abs(mix_stft))\n",
    "\n",
    "            # Reconstruct the complex spectrogram\n",
    "            sep_output=SpectrogramReconstructor().reconstruct(\n",
    "                output['mag_mask'], output['real_mask'], output['imag_mask'], mix_stft)\n",
    "            # percussion_sep = istft(sep_output, n_fft=256, hop_length=64)\n",
    "\n",
    "            # Calculate the loss\n",
    "            # loss = criterion(percussion_sep, class_output, true_percussion, true_class)\n",
    "            loss=criterion(sep_output, class_output, true_percussion_stft,\n",
    "                           true_class, alpha=0.7, beta=0.3, spectrogram_loss=True)\n",
    "\n",
    "            # Calculate the classification accuracy\n",
    "            predicted=(F.sigmoid(class_output) >= 0.5).float()\n",
    "            correct += (predicted == true_class).float().sum().item()\n",
    "            total += true_class.numel()\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_bar.set_description(\n",
    "                f\"Epoch {epoch + 1}/{num_epochs} Validation Loss: {val_loss/(i+1):.4f}\")\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracy=correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} Training Loss: {train_loss:.4f}, Training Accuracy: {\n",
    "          accuracy:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Save checkpoint at the end of each epoch or based on some condition\n",
    "    save_checkpoint(model, optimizer, epoch, val_loss, checkpoint_dir='checkpoint',\n",
    "                    filename='checkpoint_spectral_epoch_{}.pth'.format(epoch + 1))\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss=val_loss\n",
    "        patience=5\n",
    "        torch.save(model.state_dict(), 'best_model_spectral.pth')\n",
    "        print(\"Model improved. Saving the model\")\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

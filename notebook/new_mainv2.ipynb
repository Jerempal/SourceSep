{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4c21c5-8889-49ce-b2ba-f1ffda594007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import librosa.display\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from data.config import *\n",
    "from data.utils import *\n",
    "# from data.dataset import MixtureDataset, AudioMixtureDataset\n",
    "from data.dataset import AudioDataset\n",
    "from tqdm import tqdm\n",
    "from torchlibrosa.stft import STFT, ISTFT, magphase\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from metrics_loss import *\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "# mp.set_start_method('spawn', force=True)\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0f42ec-b0ab-465d-a30e-69e5a6e285b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.residual_block = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_c),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_c, out_c,\n",
    "                      kernel_size=3, stride=stride, padding=1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(out_c, out_c,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "        \"\"\" Shortcut Connection \"\"\"\n",
    "        self.shortcut = nn.Conv2d(\n",
    "            in_c, out_c, kernel_size=1, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.residual_block(inputs)\n",
    "        s = self.shortcut(inputs)\n",
    "\n",
    "        skip = x + s\n",
    "        return skip\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upsampling = nn.ConvTranspose2d(\n",
    "            in_c, out_c, kernel_size=2, stride=2, padding=0, dilation=1)\n",
    "        self.residual_block = ResidualBlock(\n",
    "            out_c * 2, out_c)\n",
    "        # self.upsampling = nn.Upsample(\n",
    "        #     scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        # self.residual_block = ResidualBlock(\n",
    "        #     in_c + out_c, out_c)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        # Upsample\n",
    "        x = self.upsampling(x)\n",
    "        # Ensure x and skip have the same spatial dimensions\n",
    "        if x.shape[2:] != skip.shape[2:]:\n",
    "            x = F.interpolate(\n",
    "                x, size=(skip.shape[2], skip.shape[3]), mode='bilinear', align_corners=True)\n",
    "\n",
    "        # Concatenate\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "\n",
    "        # Residual block\n",
    "        x = self.residual_block(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResUNet(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(ResUNet, self).__init__()\n",
    "\n",
    "        \"\"\" Encoder 1 \"\"\"\n",
    "        self.encoder_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(out_c, out_c,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "        \"\"\" Shortcut Connection \"\"\"\n",
    "        self.shortcut = nn.Conv2d(in_c, out_c, kernel_size=1, padding=0)\n",
    "\n",
    "        \"\"\" Encoder 2 and 3\"\"\"\n",
    "        self.encoder_block2 = ResidualBlock(\n",
    "            out_c, out_c * 2, stride=2)\n",
    "        self.encoder_block3 = ResidualBlock(\n",
    "            out_c * 2, out_c * 4, stride=2)\n",
    "\n",
    "        \"\"\" Bridge \"\"\"\n",
    "        self.bridge = ResidualBlock(\n",
    "            out_c * 4, out_c * 8, stride=2)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.decoder_block1 = DecoderBlock(out_c * 8, out_c * 4)\n",
    "        self.decoder_block2 = DecoderBlock(out_c * 4, out_c * 2)\n",
    "        self.decoder_block3 = DecoderBlock(out_c * 2, out_c)\n",
    "\n",
    "        \"\"\" Output \"\"\"\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(out_c, 3, kernel_size=1, padding=0),\n",
    "        )\n",
    "        \n",
    "        # # Classification head\n",
    "        # self.classifier = nn.Sequential(\n",
    "        #     nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        #     nn.Flatten(),\n",
    "        #     nn.Linear(out_c, num_classes),\n",
    "        #     nn.Sigmoid()\n",
    "        # )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        \"\"\" Encoder 1 \"\"\"\n",
    "        encoder1 = self.encoder_block1(x)\n",
    "        s = self.shortcut(x)\n",
    "        skip1 = encoder1 + s\n",
    "\n",
    "        \"\"\" Encoder 2 and 3 \"\"\"\n",
    "        skip2 = self.encoder_block2(skip1)\n",
    "        skip3 = self.encoder_block3(skip2)\n",
    "\n",
    "        \"\"\" Bridge \"\"\"\n",
    "        bridge = self.bridge(skip3)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        decoder1 = self.decoder_block1(bridge, skip3)\n",
    "        decoder2 = self.decoder_block2(decoder1, skip2)\n",
    "        decoder3 = self.decoder_block3(decoder2, skip1)\n",
    "\n",
    "        \"\"\" Output \"\"\"\n",
    "        output = self.output(decoder3)\n",
    "\n",
    "        # return output, skip3\n",
    "\n",
    "        output_masks_dict = {\n",
    "            'mag_mask': torch.sigmoid(output[:, 0, :, :]),\n",
    "            'real_mask': torch.tanh(output[:, 1, :, :]),\n",
    "            'imag_mask': torch.tanh(output[:, 2, :, :])\n",
    "        }\n",
    "        return output_masks_dict, skip3\n",
    "\n",
    "        # Classification head\n",
    "        # class_output = self.classifier(decoder3)\n",
    "        # return output_masks_dict, class_output\n",
    "\n",
    "\n",
    "class MultiTaskResUNet(nn.Module):\n",
    "    def __init__(self, num_noise_classes):\n",
    "        super().__init__()\n",
    "        self.resunet = ResUNet(in_c=1, out_c=32)\n",
    "\n",
    "        # self.classifier = nn.Sequential(\n",
    "        #     nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "        #     nn.BatchNorm2d(64),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(kernel_size=2, stride=2),  # Add max pooling here\n",
    "        #     nn.Dropout(0.3),\n",
    "        #     nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "        #     nn.BatchNorm2d(32),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(kernel_size=2, stride=2),  # Add max pooling here\n",
    "        #     nn.Dropout(0.3),\n",
    "        # )\n",
    "\n",
    "        # # output classifier\n",
    "        # self.classifier_output = nn.Sequential(\n",
    "        #     nn.Linear(32 * 8 * 30, 64),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.3),\n",
    "        #     nn.Linear(64, num_noise_classes),  # Corrected the input size to 64\n",
    "        # )\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_noise_classes),\n",
    "            # nn.Linear(128, num_noise_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output, skip3 = self.resunet(x)\n",
    "\n",
    "        # x = self.classifier(skip3)\n",
    "        # x = torch.flatten(x, start_dim=1)\n",
    "        # x = self.classifier_output(x)\n",
    "\n",
    "        x = self.classifier(skip3)\n",
    "\n",
    "        return output, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1b9837-eb86-442d-adca-36a02e1efe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Define the multi-task loss function\n",
    "def multi_task_loss(separation_output, classification_output, true_percussion, true_class, alpha=0.7, beta=0.3, spectrogram_loss=False):\n",
    "    \n",
    "    if spectrogram_loss == False:\n",
    "        mse_loss = nn.MSELoss()\n",
    "        separation_loss = mse_loss(separation_output, true_percussion)\n",
    "\n",
    "    else:\n",
    "        separation_loss = spectral_loss(separation_output, true_percussion)\n",
    "        \n",
    "    # classification_loss = nn.CrossEntropyLoss()(classification_output, true_class) 1 ere version\n",
    "    # classification_loss = nn.BCELoss()(classification_output, F.one_hot(true_class, num_classes=8).float()) #2 eme version\n",
    "    \n",
    "    classification_loss = nn.BCEWithLogitsLoss()(classification_output, true_class)  # 3 eme version\n",
    "    # classification_loss = nn.BCEWithLogitsLoss()(classification_output, F.one_hot(true_class, num_classes=8).float()) #3 eme version\n",
    "\n",
    "    loss = alpha * separation_loss + beta * classification_loss\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c55d3-3d34-481a-b145-a5baf8fdb014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Load metadata\n",
    "metadata = pd.read_csv(os.path.join(\n",
    "    DATASET_MIX_AUDIO_PATH, \"metadata.csv\"))\n",
    "\n",
    "# define the train, validation and test sets\n",
    "\n",
    "# dataset = MixtureDataset(metadata_file=metadata, k=0.6,\n",
    "#                          noise_class=None)\n",
    "# dataset = AudioMixtureDataset(metadata_file=metadata, k=0.4,\n",
    "#                               noise_class='siren')\n",
    "# dataset = AudioMixtureDataset(metadata_file=metadata, k=None, noise_class=None)\n",
    "\n",
    "dataset = AudioDataset(metadata_file=metadata, noise_classes=[\n",
    "                                          'engine_idling', 'air_conditioner'], random_noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b5a403-75cd-4ee3-b601-4669da266997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# when using the saved indices\n",
    "# train_indices = np.load('train_indices.npy')\n",
    "# val_indices = np.load('val_indices.npy')\n",
    "# test_indices = np.load('test_indices.npy')\n",
    "\n",
    "train_indices = np.load('train_indices_engine_air.npy')\n",
    "val_indices = np.load('val_indices_engine_air.npy')\n",
    "test_indices = np.load('test_indices_engine_air.npy')\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "# train_loader = DataLoader(dataset, sampler=train_sampler, batch_size=32, num_workers=2, persistent_workers=True, prefetch_factor=2)\n",
    "# val_loader = DataLoader(dataset, sampler=val_sampler,\n",
    "#                         batch_size=32, num_workers=2, persistent_workers=True, prefetch_factor=2)\n",
    "# test_loader = DataLoader(dataset, sampler=test_sampler,\n",
    "#                          batch_size=32, num_workers=2, persistent_workers=True, prefetch_factor=2)\n",
    "\n",
    "train_loader = DataLoader(dataset, sampler=train_sampler, batch_size=32)\n",
    "val_loader = DataLoader(dataset, sampler=val_sampler, batch_size=32)\n",
    "test_loader = DataLoader(dataset, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769c99e6-62cc-46bf-ae67-802c75d2ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "class SpectrogramReconstructor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def magphase(self, real, imag):\n",
    "        mag = (real ** 2 + imag ** 2) ** 0.5\n",
    "        cos = real / torch.clamp(mag, 1e-10, np.inf)\n",
    "        sin = imag / torch.clamp(mag, 1e-10, np.inf)\n",
    "        \n",
    "        return mag, cos, sin\n",
    "\n",
    "    def reconstruct(self, mag_mask, real_mask, imag_mask, mix_stft):\n",
    "\n",
    "        mix_mag, mix_cos, mix_sin = self.magphase(mix_stft.real, mix_stft.imag)\n",
    "        _, mask_cos, mask_sin = self.magphase(real_mask, imag_mask)\n",
    "\n",
    "        # calculate the |Y| = |M| * |X|\n",
    "        estimated_mag = mag_mask * mix_mag\n",
    "\n",
    "        # Reconstruct the complex spectrogram\n",
    "        Y_real = estimated_mag * (mask_cos * mix_cos - mask_sin * mix_sin)\n",
    "        Y_imag = estimated_mag * (mask_cos * mix_sin + mask_sin * mix_cos)\n",
    "        sep_output = torch.complex(Y_real, Y_imag)\n",
    "\n",
    "        return sep_output\n",
    "\n",
    "\n",
    "# ISTFT conversion function\n",
    "\n",
    "\n",
    "def istft(sep_output, n_fft, hop_length):\n",
    "\n",
    "    y = torch.istft(\n",
    "        sep_output, n_fft, hop_length, window=torch.hann_window(256, device='cuda'), length=31248)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c43b21-abf4-4176-a70a-04309277a26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Define the model, optimizer and loss function\n",
    "model = MultiTaskResUNet(num_noise_classes=8).to(\"cuda\")\n",
    "# optimizer = AdamW(model.parameters(), lr=0.001, amsgrad=True)\n",
    "optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "criterion = multi_task_loss\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e389da-d299-4b2a-9ef0-e5947985803c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Training Loss: 1.2401: 100%|\u001b[32m██████████\u001b[0m| 363/363 [06:30<00:00,  1.08s/it]\n",
      "Epoch 1/10 Validation Loss: 1.1733: 100%|\u001b[31m██████████\u001b[0m| 121/121 [01:20<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Training Loss: 1.2401, Training Accuracy: 0.9353, Validation Loss: 1.1733, Validation Accuracy: 0.9376\n",
      "Checkpoint saved at 'checkpoint\\checkpoint_air_engine_spectral_epoch_1.pth'\n",
      "Model improved. Saving the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Training Loss: 1.1626: 100%|\u001b[32m██████████\u001b[0m| 363/363 [08:58<00:00,  1.48s/it]\n",
      "Epoch 2/10 Validation Loss: 1.1790: 100%|\u001b[31m██████████\u001b[0m| 121/121 [01:21<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Training Loss: 1.1626, Training Accuracy: 0.9369, Validation Loss: 1.1790, Validation Accuracy: 0.9379\n",
      "Checkpoint saved at 'checkpoint\\checkpoint_air_engine_spectral_epoch_2.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 Training Loss: 1.1365: 100%|\u001b[32m██████████\u001b[0m| 363/363 [09:22<00:00,  1.55s/it]\n",
      "Epoch 3/10 Validation Loss: 1.1113: 100%|\u001b[31m██████████\u001b[0m| 121/121 [01:21<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 Training Loss: 1.1365, Training Accuracy: 0.9372, Validation Loss: 1.1113, Validation Accuracy: 0.9376\n",
      "Checkpoint saved at 'checkpoint\\checkpoint_air_engine_spectral_epoch_3.pth'\n",
      "Model improved. Saving the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 Training Loss: 1.1113: 100%|\u001b[32m██████████\u001b[0m| 363/363 [09:04<00:00,  1.50s/it]\n",
      "Epoch 4/10 Validation Loss: 1.1863: 100%|\u001b[31m██████████\u001b[0m| 121/121 [01:22<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 Training Loss: 1.1113, Training Accuracy: 0.9374, Validation Loss: 1.1863, Validation Accuracy: 0.9379\n",
      "Checkpoint saved at 'checkpoint\\checkpoint_air_engine_spectral_epoch_4.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 Training Loss: 1.0398:   0%|\u001b[32m          \u001b[0m| 1/363 [00:02<12:36,  2.09s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m total \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     19\u001b[0m train_bar \u001b[39m=\u001b[39m tqdm(train_loader, desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m                 \u001b[39mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m Training Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, colour\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgreen\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_bar):\n\u001b[0;32m     22\u001b[0m     \u001b[39m# Zero the parameter gradients\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     25\u001b[0m     \u001b[39m# Move data to device\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jejep\\anaconda3\\envs\\ProjectEnv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jejep\\anaconda3\\envs\\ProjectEnv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:700\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    698\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    699\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 700\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[0;32m    701\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    702\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    703\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable\n\u001b[0;32m    704\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    705\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    706\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\jejep\\anaconda3\\envs\\ProjectEnv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:756\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    754\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    755\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 756\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    757\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    758\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\jejep\\anaconda3\\envs\\ProjectEnv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\jejep\\Desktop\\STAGE\\Projects\\SourceSep\\data\\dataset.py:126\u001b[0m, in \u001b[0;36mAudioDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    122\u001b[0m noise_row \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata[\u001b[39m'\u001b[39m\u001b[39mnoise_class\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    123\u001b[0m                             \u001b[39m==\u001b[39m noise_class]\u001b[39m.\u001b[39msample()\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m]\n\u001b[0;32m    124\u001b[0m noise_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(DATASET_NOISE_PATH, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfold\u001b[39m\u001b[39m{\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[39m                          \u001b[39mnoise_row[\u001b[39m'\u001b[39m\u001b[39mfold\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, noise_row[\u001b[39m'\u001b[39m\u001b[39mnoise_file\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 126\u001b[0m noise_audio \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_and_pad_audio(noise_path)\n\u001b[0;32m    127\u001b[0m noise_waveforms\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_normalize_loudness(noise_audio))\n\u001b[0;32m    129\u001b[0m \u001b[39m# Set the noise label for the current noise class\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jejep\\Desktop\\STAGE\\Projects\\SourceSep\\data\\dataset.py:38\u001b[0m, in \u001b[0;36mAudioDataset._load_and_pad_audio\u001b[1;34m(self, audio_path)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_and_pad_audio\u001b[39m(\u001b[39mself\u001b[39m, audio_path):\n\u001b[1;32m---> 38\u001b[0m     \u001b[39mreturn\u001b[39;00m pad_audio_center(audio_path)\n",
      "File \u001b[1;32mc:\\Users\\jejep\\Desktop\\STAGE\\Projects\\SourceSep\\data\\utils.py:46\u001b[0m, in \u001b[0;36mpad_audio_center\u001b[1;34m(audio_path, sample_rate, target_length)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpad_audio_center\u001b[39m(audio_path, sample_rate\u001b[39m=\u001b[39m\u001b[39m7812\u001b[39m, target_length\u001b[39m=\u001b[39m\u001b[39m31248\u001b[39m):\n\u001b[1;32m---> 46\u001b[0m     audio, sr \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39mload(audio_path, sr\u001b[39m=\u001b[39msample_rate)\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(audio) \u001b[39m<\u001b[39m target_length:\n\u001b[0;32m     49\u001b[0m         pad_len \u001b[39m=\u001b[39m (target_length \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(audio)) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jejep\\anaconda3\\envs\\ProjectEnv\\Lib\\site-packages\\librosa\\core\\audio.py:190\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39m# Final cleanup for dtype and contiguity\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m mono:\n\u001b[1;32m--> 190\u001b[0m     y \u001b[39m=\u001b[39m to_mono(y)\n\u001b[0;32m    192\u001b[0m \u001b[39mif\u001b[39;00m sr \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     y \u001b[39m=\u001b[39m resample(y, orig_sr\u001b[39m=\u001b[39msr_native, target_sr\u001b[39m=\u001b[39msr, res_type\u001b[39m=\u001b[39mres_type)\n",
      "File \u001b[1;32mc:\\Users\\jejep\\anaconda3\\envs\\ProjectEnv\\Lib\\site-packages\\librosa\\core\\audio.py:508\u001b[0m, in \u001b[0;36mto_mono\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    505\u001b[0m util\u001b[39m.\u001b[39mvalid_audio(y, mono\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    507\u001b[0m \u001b[39mif\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 508\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(y, axis\u001b[39m=\u001b[39m\u001b[39mtuple\u001b[39m(\u001b[39mrange\u001b[39m(y\u001b[39m.\u001b[39mndim \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)))\n\u001b[0;32m    510\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\jejep\\anaconda3\\envs\\ProjectEnv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3501\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3502\u001b[0m         \u001b[39mreturn\u001b[39;00m mean(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 3504\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39m_mean(a, axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m   3505\u001b[0m                       out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jejep\\anaconda3\\envs\\ProjectEnv\\Lib\\site-packages\\numpy\\core\\_methods.py:118\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    115\u001b[0m         dtype \u001b[39m=\u001b[39m mu\u001b[39m.\u001b[39mdtype(\u001b[39m'\u001b[39m\u001b[39mf4\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    116\u001b[0m         is_float16_result \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m ret \u001b[39m=\u001b[39m umr_sum(arr, axis, dtype, out, keepdims, where\u001b[39m=\u001b[39mwhere)\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ret, mu\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m    120\u001b[0m     \u001b[39mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Train the model\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = np.inf\n",
    "patience = 5\n",
    "num_epochs = 10\n",
    "\n",
    "# model, optimizer, start_epoch, loss = load_checkpoint(model, optimizer, checkpoint_dir='checkpoint', filename='checkpoint_air_engine_epoch_3.pth')\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {\n",
    "                     epoch + 1}/{num_epochs} Training Loss: {train_loss:.4f}\", colour='green')\n",
    "    for i, batch in enumerate(train_bar):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move data to device\n",
    "        mixture = batch['mixture_audio'].to(device)\n",
    "        mix_stft = batch['mix_stft'].to(device)\n",
    "        true_percussion = batch['percussion_audio'].to(device)\n",
    "        true_percussion_stft = batch['perc_stft'].to(device)\n",
    "        \n",
    "        # true_class = batch['noise_class'].to(device)\n",
    "        # ici true class est un tensor de taille (batch_size, 8) avec des 0 et des 1 pour les classes présentes et absentes\n",
    "        true_class = batch['noise_labels'].to(device)\n",
    "\n",
    "        # Calculate real and imaginary parts of the mixture\n",
    "        # mix_stft = torch.stft(mixture, n_fft=256, hop_length=64, win_length=256, window=torch.hann_window(\n",
    "        #     window_length=256, device=device), return_complex=True)\n",
    "        # true_percussion_stft = torch.stft(true_percussion, n_fft=256, hop_length=64, win_length=256, window=torch.hann_window(\n",
    "        #     window_length=256, device=device), return_complex=True)\n",
    "        \n",
    "        # Forward pass\n",
    "        output, class_output = model(torch.abs(mix_stft))\n",
    "\n",
    "        # mag_mask = torch.sigmoid(output[:, 0, :, :])\n",
    "        # real_mask = torch.tanh(output[:, 1, :, :])\n",
    "        # imag_mask = torch.tanh(output[:, 2, :, :])\n",
    "        # ^^^ output est déjà un dict avec mag_mask, real_mask, imag_mask\n",
    "        \n",
    "        # Reconstruct the complex spectrogram\n",
    "        sep_output = SpectrogramReconstructor().reconstruct(\n",
    "            output['mag_mask'], output['real_mask'], output['imag_mask'], mix_stft)\n",
    "        # percussion_sep = istft(sep_output, n_fft=256, hop_length=64)\n",
    "        \n",
    "        # else we calculate log spectral loss so we need to calculate the stft of the separated percussion (sep_output is the complex spectrogram of the separated percussion)\n",
    "        # true_percussion_stft = torch.stft(true_percussion, n_fft=256, hop_length=64, win_length=256, window=torch.hann_window(window_length=256, device=device), return_complex=True)\n",
    "\n",
    "        predicted = (class_output > 0.5).float()\n",
    "        # Somme des prédictions correctes\n",
    "        correct += (predicted == true_class).float().sum().item()\n",
    "        total += true_class.numel()  # total doit compter tous les éléments dans true_class\n",
    "\n",
    "        # Calculate the loss\n",
    "        # loss = criterion(percussion_sep, class_output, true_percussion, true_class)\n",
    "        loss = criterion(sep_output, class_output, true_percussion_stft, true_class, alpha=0.7, beta=0.3, spectrogram_loss=True)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        train_bar.set_description(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs} Training Loss: {train_loss/(i+1):.4f}\")\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    val_bar = tqdm(val_loader, desc=f\"Epoch {\n",
    "                   epoch + 1}/{num_epochs} Validation Loss: {val_loss:.4f}\", colour='red')\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_bar):\n",
    "            # Move data to device\n",
    "            mixture = batch['mixture_audio'].to(device)\n",
    "            mix_stft = batch['mix_stft'].to(device)\n",
    "            true_percussion = batch['percussion_audio'].to(device)\n",
    "            true_percussion_stft = batch['perc_stft'].to(device)\n",
    "            true_class = batch['noise_labels'].to(device)\n",
    "\n",
    "            # Calculate real and imaginary parts of the mixture\n",
    "            # mix_stft = torch.stft(mixture, n_fft=256, hop_length=64, win_length=256, window=torch.hann_window(\n",
    "            #     window_length=256, device=device), return_complex=True)\n",
    "\n",
    "            # Forward pass\n",
    "            output, class_output = model(torch.abs(mix_stft))\n",
    "\n",
    "            # mag_mask = torch.sigmoid(output[:, 0, :, :])\n",
    "            # real_mask = torch.tanh(output[:, 1, :, :])\n",
    "            # imag_mask = torch.tanh(output[:, 2, :, :])\n",
    "            # ^^^ output is already a dictionary with keys mag_mask, real_mask, imag_mask\n",
    "\n",
    "            # Reconstruct the complex spectrogram\n",
    "            sep_output = SpectrogramReconstructor().reconstruct(\n",
    "                output['mag_mask'], output['real_mask'], output['imag_mask'], mix_stft)\n",
    "            # percussion_sep = istft(sep_output, n_fft=256, hop_length=64)\n",
    "\n",
    "            # Calculate the classification accuracy\n",
    "            predicted = (class_output > 0.5).float()\n",
    "            correct += (predicted == true_class).float().sum().item()\n",
    "            total += true_class.numel()\n",
    "\n",
    "            # Calculate the loss\n",
    "            # loss = criterion(percussion_sep, class_output, true_percussion, true_class)\n",
    "            loss = criterion(sep_output, class_output, true_percussion_stft, true_class, alpha=0.7, beta=0.3, spectrogram_loss=True)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_bar.set_description(\n",
    "                f\"Epoch {epoch + 1}/{num_epochs} Validation Loss: {val_loss/(i+1):.4f}\")\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracy = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} Training Loss: {train_loss:.4f}, Training Accuracy: {\n",
    "          accuracy:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Save checkpoint at the end of each epoch or based on some condition\n",
    "    save_checkpoint(model, optimizer, epoch, val_loss, checkpoint_dir='checkpoint',\n",
    "                    filename='checkpoint_air_engine_spectral_epoch_{}.pth'.format(epoch + 1))\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience = 5\n",
    "        torch.save(model.state_dict(), 'best_model_air_engine_spectral.pth')\n",
    "        print(\"Model improved. Saving the model\")\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

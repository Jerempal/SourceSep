{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac5eafa-e955-4272-ac71-9619a49db88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "import torchsummary as summary\n",
    "import librosa.display\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numba as nb\n",
    "import dask as dk\n",
    "import joblib as jl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from data.config import *\n",
    "from data.utils import *\n",
    "# from data.dataset import MixtureDataset, AudioMixtureDataset\n",
    "from data.dataset import AudioMixtureDatasetWithLoudnorm\n",
    "from tqdm import tqdm\n",
    "from torchlibrosa.stft import STFT, ISTFT, magphase\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "# mp.set_start_method('spawn', force=True)\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df988aa5-f35e-4ef5-8b01-e13aaef19fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.residual_block = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_c),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_c, out_c,\n",
    "                      kernel_size=3, stride=stride, padding=1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(out_c, out_c,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "        \"\"\" Shortcut Connection \"\"\"\n",
    "        self.shortcut = nn.Conv2d(\n",
    "            in_c, out_c, kernel_size=1, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.residual_block(inputs)\n",
    "        s = self.shortcut(inputs)\n",
    "\n",
    "        skip = x + s\n",
    "        return skip\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upsampling = nn.ConvTranspose2d(\n",
    "            in_c, out_c, kernel_size=2, stride=2, padding=0, dilation=1)\n",
    "        self.residual_block = ResidualBlock(\n",
    "            out_c * 2, out_c)\n",
    "        # self.upsampling = nn.Upsample(\n",
    "        #     scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        # self.residual_block = ResidualBlock(\n",
    "        #     in_c + out_c, out_c)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        # Upsample\n",
    "        x = self.upsampling(x)\n",
    "        # Ensure x and skip have the same spatial dimensions\n",
    "        if x.shape[2:] != skip.shape[2:]:\n",
    "            x = F.interpolate(\n",
    "                x, size=(skip.shape[2], skip.shape[3]), mode='bilinear', align_corners=True)\n",
    "\n",
    "        # Concatenate\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "\n",
    "        # Residual block\n",
    "        x = self.residual_block(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResUNet(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super(ResUNet, self).__init__()\n",
    "\n",
    "        \"\"\" Encoder 1 \"\"\"\n",
    "        self.encoder_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(out_c, out_c,\n",
    "                      kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "        \"\"\" Shortcut Connection \"\"\"\n",
    "        self.shortcut = nn.Conv2d(in_c, out_c, kernel_size=1, padding=0)\n",
    "\n",
    "        \"\"\" Encoder 2 and 3\"\"\"\n",
    "        self.encoder_block2 = ResidualBlock(\n",
    "            out_c, out_c * 2, stride=2)\n",
    "        self.encoder_block3 = ResidualBlock(\n",
    "            out_c * 2, out_c * 4, stride=2)\n",
    "\n",
    "        \"\"\" Bridge \"\"\"\n",
    "        self.bridge = ResidualBlock(\n",
    "            out_c * 4, out_c * 8, stride=2)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.decoder_block1 = DecoderBlock(out_c * 8, out_c * 4)\n",
    "        self.decoder_block2 = DecoderBlock(out_c * 4, out_c * 2)\n",
    "        self.decoder_block3 = DecoderBlock(out_c * 2, out_c)\n",
    "\n",
    "        \"\"\" Output \"\"\"\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(out_c, 3, kernel_size=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "\n",
    "        \"\"\" Encoder 1 \"\"\"\n",
    "        encoder1 = self.encoder_block1(inputs)\n",
    "        s = self.shortcut(inputs)\n",
    "        skip1 = encoder1 + s\n",
    "\n",
    "        \"\"\" Encoder 2 and 3 \"\"\"\n",
    "        skip2 = self.encoder_block2(skip1)\n",
    "        skip3 = self.encoder_block3(skip2)\n",
    "\n",
    "        \"\"\" Bridge \"\"\"\n",
    "        bridge = self.bridge(skip3)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        decoder1 = self.decoder_block1(bridge, skip3)\n",
    "        decoder2 = self.decoder_block2(decoder1, skip2)\n",
    "        decoder3 = self.decoder_block3(decoder2, skip1)\n",
    "\n",
    "        \"\"\" Output \"\"\"\n",
    "        output = self.output(decoder3)\n",
    "\n",
    "        # return output, skip3\n",
    "\n",
    "        output_masks_dict = {\n",
    "            'mag_mask': torch.sigmoid(output[:, 0, :, :]),\n",
    "            'real_mask': torch.tanh(output[:, 1, :, :]),\n",
    "            'imag_mask': torch.tanh(output[:, 2, :, :])\n",
    "        }\n",
    "\n",
    "        return output_masks_dict, skip3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed44d6c7-8adb-4387-971a-53feef324377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "class MultiTaskResUNet(nn.Module):\n",
    "    def __init__(self, num_noise_classes):\n",
    "        super().__init__()\n",
    "        self.resunet = ResUNet(in_c=1, out_c=32)\n",
    "\n",
    "        # self.classifier = nn.Sequential(\n",
    "        #     nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "        #     nn.BatchNorm2d(64),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(kernel_size=2, stride=2),  # Add max pooling here\n",
    "        #     nn.Dropout(0.3),\n",
    "        #     nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "        #     nn.BatchNorm2d(32),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(kernel_size=2, stride=2),  # Add max pooling here\n",
    "        #     nn.Dropout(0.3),\n",
    "        # )\n",
    "\n",
    "        # # output classifier\n",
    "        # self.classifier_output = nn.Sequential(\n",
    "        #     nn.Linear(32 * 8 * 30, 64),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.3),\n",
    "        #     nn.Linear(64, num_noise_classes),  # Corrected the input size to 64\n",
    "        # )\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_noise_classes),\n",
    "            # nn.Linear(128, num_noise_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output, skip3 = self.resunet(x)\n",
    "\n",
    "        # x = self.classifier(skip3)\n",
    "        # x = torch.flatten(x, start_dim=1)\n",
    "        # x = self.classifier_output(x)\n",
    "\n",
    "        x = self.classifier(skip3)\n",
    "\n",
    "        return output, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c62b133-fc90-493f-89cd-50aa780ac774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Define the multi-task loss function\n",
    "def multi_task_loss(separation_output, classification_output, true_percussion, true_class, alpha=0.7, beta=0.3):\n",
    "    mse_loss = nn.MSELoss()\n",
    "\n",
    "    separation_loss = mse_loss(separation_output, true_percussion)\n",
    "    # classification_loss = nn.CrossEntropyLoss()(classification_output, true_class) 1 ere version\n",
    "    # classification_loss = nn.BCELoss()(classification_output, F.one_hot(true_class, num_classes=8).float()) #2 eme version\n",
    "    \n",
    "    classification_loss = nn.BCEWithLogitsLoss()(classification_output, true_class)  # 3 eme version\n",
    "    # classification_loss = nn.BCEWithLogitsLoss()(classification_output, F.one_hot(true_class, num_classes=8).float()) #3 eme version\n",
    "\n",
    "    loss = alpha * separation_loss + beta * classification_loss\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e091740-d39a-46a1-b42d-bc7a8eb77c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Load metadata\n",
    "metadata = pd.read_csv(os.path.join(\n",
    "    DATASET_MIX_AUDIO_PATH, \"metadata.csv\"))\n",
    "\n",
    "# define the train, validation and test sets\n",
    "\n",
    "# dataset = MixtureDataset(metadata_file=metadata, k=0.6,\n",
    "#                          noise_class=None)\n",
    "# dataset = AudioMixtureDataset(metadata_file=metadata, k=0.4,\n",
    "#                               noise_class='siren')\n",
    "# dataset = AudioMixtureDataset(metadata_file=metadata, k=None, noise_class=None)\n",
    "\n",
    "dataset = AudioMixtureDatasetWithLoudnorm(metadata_file=metadata, noise_classes=[\n",
    "                                          'engine_idling', 'air_conditioner'], random_noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310fe5c5-5866-4b3f-a401-62440a00d9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# when using the saved indices\n",
    "# train_indices = np.load('train_indices.npy')\n",
    "# val_indices = np.load('val_indices.npy')\n",
    "# test_indices = np.load('test_indices.npy')\n",
    "train_indices = np.load('train_indices_engine_air.npy')\n",
    "val_indices = np.load('val_indices_engine_air.npy')\n",
    "test_indices = np.load('test_indices_engine_air.npy')\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "# train_loader = DataLoader(dataset, sampler=train_sampler, batch_size=64, num_workers=2, persistent_workers=True, prefetch_factor=2)\n",
    "# val_loader = DataLoader(dataset, sampler=val_sampler,\n",
    "#                         batch_size=128, num_workers=2, persistent_workers=True, prefetch_factor=2)\n",
    "# test_loader = DataLoader(dataset, sampler=test_sampler,\n",
    "#                          batch_size=128, num_workers=2, persistent_workers=True, prefetch_factor=2)\n",
    "\n",
    "train_loader = DataLoader(dataset, sampler=train_sampler, batch_size=32)\n",
    "val_loader = DataLoader(dataset, sampler=val_sampler, batch_size=32)\n",
    "test_loader = DataLoader(dataset, sampler=test_sampler, batch_size=32)\n",
    "\n",
    "# data = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6516a8a4-65d8-43f7-91e8-913cc1c441ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "class SpectrogramReconstructor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def magphase(self, real, imag):\n",
    "        mag = (real ** 2 + imag ** 2) ** 0.5\n",
    "        cos = real / torch.clamp(mag, 1e-10, np.inf)\n",
    "        sin = imag / torch.clamp(mag, 1e-10, np.inf)\n",
    "        \n",
    "        return mag, cos, sin\n",
    "\n",
    "    def reconstruct(self, mag_mask, real_mask, imag_mask, mix_stft):\n",
    "        \n",
    "        mix_mag, mix_cos, mix_sin = self.magphase(mix_stft.real, mix_stft.imag)\n",
    "        _, mask_cos, mask_sin = self.magphase(real_mask, imag_mask)\n",
    "        \n",
    "        # calculate the |Y| = |M| * |X|\n",
    "        estimated_mag = mag_mask * mix_mag\n",
    "\n",
    "        # Reconstruct the complex spectrogram\n",
    "        Y_real = estimated_mag * (mask_cos * mix_cos - mask_sin * mix_sin)\n",
    "        Y_imag = estimated_mag * (mask_cos * mix_sin + mask_sin * mix_cos)\n",
    "        Y_complex = torch.complex(Y_real, Y_imag)\n",
    "\n",
    "        return Y_complex\n",
    "\n",
    "\n",
    "# ISTFT conversion function\n",
    "\n",
    "\n",
    "def istft(y_complex, n_fft, hop_length):\n",
    "\n",
    "    y = torch.istft(\n",
    "        y_complex, n_fft, hop_length, window=torch.hann_window(256, device='cuda'), length=31248)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb5728-d7e0-4ac9-b5d9-313b3f8e9649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Define the model, optimizer and loss function\n",
    "model = MultiTaskResUNet(num_noise_classes=8).to(\"cuda\")\n",
    "# optimizer = AdamW(model.parameters(), lr=0.001, amsgrad=True)\n",
    "optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "criterion = multi_task_loss\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc7a7cd-1f9e-4f9b-b66a-a2a293bd01fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Training Loss: 0.1994: 100%|\u001b[32m██████████\u001b[0m| 363/363 [05:09<00:00,  1.17it/s]\n",
      "Epoch 1/10 Validation Loss: 0.1984: 100%|\u001b[31m██████████\u001b[0m| 121/121 [01:00<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Training Loss: 0.1994, Training Accuracy: 0.9355, Validation Loss: 0.1984, Validation Accuracy: 0.9382\n",
      "Checkpoint saved at 'checkpoint\\checkpoint_air_engine_epoch_1.pth'\n",
      "Model improved. Saving the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Training Loss: 0.1988: 100%|\u001b[32m██████████\u001b[0m| 363/363 [06:54<00:00,  1.14s/it]\n",
      "Epoch 2/10 Validation Loss: 0.1985: 100%|\u001b[31m██████████\u001b[0m| 121/121 [00:58<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 Training Loss: 0.1988, Training Accuracy: 0.9370, Validation Loss: 0.1985, Validation Accuracy: 0.9377\n",
      "Checkpoint saved at 'checkpoint\\checkpoint_air_engine_epoch_2.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 Training Loss: 0.1986: 100%|\u001b[32m██████████\u001b[0m| 363/363 [06:59<00:00,  1.16s/it]\n",
      "Epoch 3/10 Validation Loss: 0.1984: 100%|\u001b[31m██████████\u001b[0m| 121/121 [00:58<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 Training Loss: 0.1986, Training Accuracy: 0.9378, Validation Loss: 0.1984, Validation Accuracy: 0.9382\n",
      "Checkpoint saved at 'checkpoint\\checkpoint_air_engine_epoch_3.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 Training Loss: 0.1964:   2%|\u001b[32m▏         \u001b[0m| 8/363 [00:09<07:15,  1.23s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jejep\\Desktop\\STAGE\\Projects\\SourceSep\\new_main.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m total \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     19\u001b[0m train_bar \u001b[39m=\u001b[39m tqdm(train_loader, desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m                 \u001b[39mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m Training Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, colour\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgreen\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_bar):\n\u001b[0;32m     22\u001b[0m     \u001b[39m# Zero the parameter gradients\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     25\u001b[0m     \u001b[39m# Move data to device\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jejep\\anaconda3\\envs\\ProjectEnv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jejep\\anaconda3\\envs\\ProjectEnv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:700\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    698\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    699\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 700\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[0;32m    701\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    702\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    703\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable\n\u001b[0;32m    704\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    705\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    706\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\jejep\\anaconda3\\envs\\ProjectEnv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:756\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    754\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    755\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 756\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    757\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    758\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\jejep\\anaconda3\\envs\\ProjectEnv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\jejep\\Desktop\\STAGE\\Projects\\SourceSep\\data\\dataset.py:129\u001b[0m, in \u001b[0;36mAudioMixtureDatasetWithLoudnorm.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    125\u001b[0m noise_row \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata[\u001b[39m'\u001b[39m\u001b[39mnoise_class\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m noise_class]\u001b[39m.\u001b[39msample(\n\u001b[0;32m    126\u001b[0m     n\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m]\n\u001b[0;32m    127\u001b[0m noise_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(DATASET_NOISE_PATH, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfold\u001b[39m\u001b[39m{\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m                          \u001b[39mnoise_row[\u001b[39m'\u001b[39m\u001b[39mfold\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, noise_row[\u001b[39m'\u001b[39m\u001b[39mnoise_file\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 129\u001b[0m noise_audio \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_and_pad_audio(noise_path)\n\u001b[0;32m    130\u001b[0m noise_waveforms\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_normalize_loudness(noise_audio))\n\u001b[0;32m    132\u001b[0m \u001b[39m# Set the noise label for the current noise class\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jejep\\Desktop\\STAGE\\Projects\\SourceSep\\data\\dataset.py:38\u001b[0m, in \u001b[0;36mAudioMixtureDatasetWithLoudnorm._load_and_pad_audio\u001b[1;34m(self, audio_path)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_and_pad_audio\u001b[39m(\u001b[39mself\u001b[39m, audio_path):\n\u001b[1;32m---> 38\u001b[0m     \u001b[39mreturn\u001b[39;00m pad_audio_center(audio_path)\n",
      "File \u001b[1;32mc:\\Users\\jejep\\Desktop\\STAGE\\Projects\\SourceSep\\data\\utils.py:46\u001b[0m, in \u001b[0;36mpad_audio_center\u001b[1;34m(audio_path, sample_rate, target_length)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpad_audio_center\u001b[39m(audio_path, sample_rate\u001b[39m=\u001b[39m\u001b[39m7812\u001b[39m, target_length\u001b[39m=\u001b[39m\u001b[39m31248\u001b[39m):\n\u001b[1;32m---> 46\u001b[0m     audio, sr \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39mload(audio_path, sr\u001b[39m=\u001b[39msample_rate)\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(audio) \u001b[39m<\u001b[39m target_length:\n\u001b[0;32m     49\u001b[0m         pad_len \u001b[39m=\u001b[39m (target_length \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(audio)) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\jejep\\anaconda3\\envs\\ProjectEnv\\Lib\\site-packages\\librosa\\core\\audio.py:176\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m     \u001b[39m# Otherwise try soundfile first, and then fall back if necessary\u001b[39;00m\n\u001b[0;32m    175\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m         y, sr_native \u001b[39m=\u001b[39m __soundfile_load(path, offset, duration, dtype)\n\u001b[0;32m    178\u001b[0m     \u001b[39mexcept\u001b[39;00m sf\u001b[39m.\u001b[39mSoundFileRuntimeError \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    179\u001b[0m         \u001b[39m# If soundfile failed, try audioread instead\u001b[39;00m\n\u001b[0;32m    180\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, (\u001b[39mstr\u001b[39m, pathlib\u001b[39m.\u001b[39mPurePath)):\n",
      "File \u001b[1;32mc:\\Users\\jejep\\anaconda3\\envs\\ProjectEnv\\Lib\\site-packages\\librosa\\core\\audio.py:222\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    219\u001b[0m         frame_duration \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m    221\u001b[0m     \u001b[39m# Load the target number of frames, and transpose to match librosa form\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     y \u001b[39m=\u001b[39m sf_desc\u001b[39m.\u001b[39mread(frames\u001b[39m=\u001b[39mframe_duration, dtype\u001b[39m=\u001b[39mdtype, always_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mT\n\u001b[0;32m    224\u001b[0m \u001b[39mreturn\u001b[39;00m y, sr_native\n",
      "File \u001b[1;32mc:\\Users\\jejep\\anaconda3\\envs\\ProjectEnv\\Lib\\site-packages\\soundfile.py:895\u001b[0m, in \u001b[0;36mSoundFile.read\u001b[1;34m(self, frames, dtype, always_2d, fill_value, out)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[39mif\u001b[39;00m frames \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m frames \u001b[39m>\u001b[39m \u001b[39mlen\u001b[39m(out):\n\u001b[0;32m    894\u001b[0m         frames \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(out)\n\u001b[1;32m--> 895\u001b[0m frames \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_array_io(\u001b[39m'\u001b[39m\u001b[39mread\u001b[39m\u001b[39m'\u001b[39m, out, frames)\n\u001b[0;32m    896\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m>\u001b[39m frames:\n\u001b[0;32m    897\u001b[0m     \u001b[39mif\u001b[39;00m fill_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jejep\\anaconda3\\envs\\ProjectEnv\\Lib\\site-packages\\soundfile.py:1344\u001b[0m, in \u001b[0;36mSoundFile._array_io\u001b[1;34m(self, action, array, frames)\u001b[0m\n\u001b[0;32m   1342\u001b[0m \u001b[39massert\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mitemsize \u001b[39m==\u001b[39m _ffi\u001b[39m.\u001b[39msizeof(ctype)\n\u001b[0;32m   1343\u001b[0m cdata \u001b[39m=\u001b[39m _ffi\u001b[39m.\u001b[39mcast(ctype \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m, array\u001b[39m.\u001b[39m__array_interface__[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m])\n\u001b[1;32m-> 1344\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cdata_io(action, cdata, ctype, frames)\n",
      "File \u001b[1;32mc:\\Users\\jejep\\anaconda3\\envs\\ProjectEnv\\Lib\\site-packages\\soundfile.py:1353\u001b[0m, in \u001b[0;36mSoundFile._cdata_io\u001b[1;34m(self, action, data, ctype, frames)\u001b[0m\n\u001b[0;32m   1351\u001b[0m     curr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtell()\n\u001b[0;32m   1352\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(_snd, \u001b[39m'\u001b[39m\u001b[39msf_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m action \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39mf_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m ctype)\n\u001b[1;32m-> 1353\u001b[0m frames \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file, data, frames)\n\u001b[0;32m   1354\u001b[0m _error_check(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_errorcode)\n\u001b[0;32m   1355\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseekable():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Train the model\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = np.inf\n",
    "patience = 5\n",
    "num_epochs = 10\n",
    "\n",
    "# model, optimizer, start_epoch, loss = load_checkpoint(model, optimizer)\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_bar = tqdm(train_loader, desc=f\"Epoch {\n",
    "                     epoch + 1}/{num_epochs} Training Loss: {train_loss:.4f}\", colour='green')\n",
    "    for i, batch in enumerate(train_bar):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move data to device\n",
    "        mixture = batch['mixture_audio'].to(device)\n",
    "        true_percussion = batch['percussion_audio'].to(device)\n",
    "        # true_class = batch['noise_class'].to(device)\n",
    "        # ici true class est un tensor de taille (batch_size, 8) avec des 0 et des 1 pour les classes présentes et absentes\n",
    "        true_class = batch['noise_labels'].to(device)\n",
    "\n",
    "        # Calculate real and imaginary parts of the mixture\n",
    "        mix_stft = torch.stft(mixture, n_fft=256, hop_length=64, win_length=256, window=torch.hann_window(\n",
    "            window_length=256, device=device), return_complex=True)\n",
    "\n",
    "        # Forward pass\n",
    "        output, class_output = model(torch.abs(mix_stft))\n",
    "\n",
    "        # mag_mask = torch.sigmoid(output[:, 0, :, :])\n",
    "        # real_mask = torch.tanh(output[:, 1, :, :])\n",
    "        # imag_mask = torch.tanh(output[:, 2, :, :])\n",
    "        # ^^^ output is already a dictionary with keys mag_mask, real_mask, imag_mask\n",
    "\n",
    "        # Reconstruct the complex spectrogram\n",
    "        Y_complex = SpectrogramReconstructor().reconstruct(\n",
    "            output['mag_mask'], output['real_mask'], output['imag_mask'], mix_stft)\n",
    "        percussion_sep = istft(Y_complex, n_fft=256, hop_length=64)\n",
    "        \n",
    "        # Calculate the classification accuracy\n",
    "        # _, predicted = torch.max(class_output, 1) predicted est un tensor de taille (btach_size) avec les indices correspondant aux classes prédites : 0, 1, 2, 3, 4, 5, 6, 7\n",
    "        # ca ne marche pas car les deux tensors n'ont pas la même taille\n",
    "        # _, predicted = torch.max(class_output, 1)\n",
    "        predicted = (class_output > 0.5).float()\n",
    "        # total += true_class.size(0)\n",
    "        # total += true_class.size(0) * true_class.size(1)  # Since it's multi-label, count total elements\n",
    "        # correct += (predicted == true_class).sum().item()  # Compare predicted and true labels\n",
    "        # correct = (predicted == true_class).sum().item()  # Somme des classes correctement prédites\n",
    "        # Somme des prédictions correctes\n",
    "        correct += (predicted == true_class).float().sum().item()\n",
    "        # total = true_class.numel()  # Nombre total d'éléments dans la matrice multi-label\n",
    "        total += true_class.numel()  # total doit compter tous les éléments dans true_class\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(percussion_sep, class_output,\n",
    "                         true_percussion, true_class)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        train_bar.set_description(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs} Training Loss: {train_loss/(i+1):.4f}\")\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    val_bar = tqdm(val_loader, desc=f\"Epoch {\n",
    "                   epoch + 1}/{num_epochs} Validation Loss: {val_loss:.4f}\", colour='red')\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_bar):\n",
    "            # Move data to device\n",
    "            mixture = batch['mixture_audio'].to(device)\n",
    "            true_percussion = batch['percussion_audio'].to(device)\n",
    "            true_class = batch['noise_labels'].to(device)\n",
    "\n",
    "            # Calculate real and imaginary parts of the mixture\n",
    "            mix_stft = torch.stft(mixture, n_fft=256, hop_length=64, win_length=256, window=torch.hann_window(\n",
    "                window_length=256, device=device), return_complex=True)\n",
    "\n",
    "            # Forward pass\n",
    "            output, class_output = model(torch.abs(mix_stft))\n",
    "\n",
    "            # mag_mask = torch.sigmoid(output[:, 0, :, :])\n",
    "            # real_mask = torch.tanh(output[:, 1, :, :])\n",
    "            # imag_mask = torch.tanh(output[:, 2, :, :])\n",
    "            # ^^^ output is already a dictionary with keys mag_mask, real_mask, imag_mask\n",
    "\n",
    "            # Reconstruct the complex spectrogram\n",
    "            Y_complex = SpectrogramReconstructor().reconstruct(\n",
    "                output['mag_mask'], output['real_mask'], output['imag_mask'], mix_stft)\n",
    "            percussion_sep = istft(Y_complex, n_fft=256, hop_length=64)\n",
    "\n",
    "            # Calculate the classification accuracy\n",
    "            # _, predicted = torch.max(class_output, 1)\n",
    "            predicted = (class_output > 0.5).float()\n",
    "            # total += true_class.size(0)\n",
    "            # correct += (predicted == true_class).sum().item()\n",
    "            # correct = (predicted == true_class).sum().item\n",
    "            # total = true_class.numel()\n",
    "            correct += (predicted == true_class).float().sum().item()\n",
    "            total += true_class.numel()\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(percussion_sep, class_output,\n",
    "                             true_percussion, true_class)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_bar.set_description(\n",
    "                f\"Epoch {epoch + 1}/{num_epochs} Validation Loss: {val_loss/(i+1):.4f}\")\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracy = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} Training Loss: {train_loss:.4f}, Training Accuracy: {\n",
    "          accuracy:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Save checkpoint at the end of each epoch or based on some condition\n",
    "    save_checkpoint(model, optimizer, epoch, val_loss, checkpoint_dir='checkpoint',\n",
    "                    filename='checkpoint_air_engine_epoch_{}.pth'.format(epoch + 1))\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience = 5\n",
    "        torch.save(model.state_dict(), 'best_model_air_engine.pth')\n",
    "        print(\"Model improved. Saving the model\")\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
